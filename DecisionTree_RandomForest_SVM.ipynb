{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_flkJoaR6TRO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor # for building the model\n",
        "\n",
        "#load/create data\n",
        "PL = pd.read_csv('results_final.csv')\n",
        "PLnormalized = pd.read_csv('results_final_normalized.csv')\n",
        "extra = PL[[\"total_red_card\",\"att_hd_goal\",\"att_pen_goal\",\"att_freekick_goal\",\"goal_fastbreak\",\"last_man_tackle\",\"own_goals\",\"pen_goals_conceded\",\"clearance_off_line\",\"penalty_save\",\"punches\",\"Income\",\"Balance\",\"fs_result\",\"season\",\"team\"]]\n",
        "\n",
        "#group the first 2 columns\n",
        "PL = PL.groupby([\"season\", \"team\"]).sum()\n",
        "PLnormalized = PLnormalized.groupby([\"season\", \"team\"]).sum()\n",
        "extra = extra.groupby([\"season\", \"team\"]).sum()\n",
        "\n",
        "\n",
        "#choose either PL, PLnormalized, or extra in the code below\n",
        "X = PL.drop('fs_result', axis=1)\n",
        "y = PL['fs_result']\n",
        "\n",
        "#split data accordingly (and consistently)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#decision tree\n",
        "#create units to store the performances\n",
        "train_acc_mean, train_acc_std = [], [] # to store the training accuracies\n",
        "test_acc_mean, test_acc_std = [], []   # to store the testing accuracies\n",
        "from tqdm import tqdm\n",
        "for d in tqdm(range(1,20)): # loop over tree depths\n",
        "    t_mae_train = []\n",
        "    t_mae = []\n",
        "    for n in range(20):\n",
        "        modelDT = DecisionTreeRegressor(max_depth=d, random_state=n) # init new model\n",
        "        modelDT = modelDT.fit(X_train, y_train) # train model\n",
        "        y_pred_train = modelDT.predict(X_train)\n",
        "        y_pred = modelDT.predict(X_test)\n",
        "        t_mae_train.append(mae(y_train, y_pred_train))\n",
        "        t_mae.append(mae(y_test, y_pred))\n",
        "\n",
        "    # append mean and std scores to appropriate lists\n",
        "    train_acc_mean.append(np.mean(t_mae_train))\n",
        "    test_acc_mean.append(np.mean(t_mae))\n",
        "    train_acc_std.append(np.std(t_mae_train))\n",
        "    test_acc_std.append(np.std(t_mae))\n",
        "\n",
        "plt.errorbar(x=np.arange(1,20), y=train_acc_mean, yerr=train_acc_std, label=\"Train set\")\n",
        "plt.errorbar(x=np.arange(1,20), y=test_acc_mean, yerr=test_acc_std, label=\"Test set\")\n",
        "plt.title(\"Decision tree model selection\")\n",
        "plt.ylabel(\"MAE\")\n",
        "plt.xlabel(\"Max depth of tree\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#random forest:\n",
        "train_acc_mean, train_acc_std = np.zeros((10, 12)), np.zeros((10, 12)) # storing it in an np array instead of a list makes it easier\n",
        "test_acc_mean, test_acc_std = np.zeros((10,12)), np.zeros((10,12))\n",
        "\n",
        "n_trees = np.arange(10,101,10)\n",
        "depths = np.arange(1,13,1)\n",
        "\n",
        "for tree_idx, n_trees1 in tqdm(enumerate(n_trees), total=len(n_trees)):\n",
        "    for d_idx, d in enumerate(depths):\n",
        "        train_perfs = []\n",
        "        test_perfs = []\n",
        "        for n in range(10):\n",
        "            modelRFC = RandomForestRegressor(n_estimators = n_trees1, max_depth=d, random_state = n)\n",
        "            modelRFC = modelRFC.fit(X_train, y_train) # train model\n",
        "            y_pred_train = modelRFC.predict(X_train)\n",
        "            y_pred = modelRFC.predict(X_test)\n",
        "            test = mae(y_train, y_pred_train)\n",
        "            train_perfs.append(mae(y_train, y_pred_train)) # store interem values\n",
        "            test_perfs.append(mae(y_test, y_pred)) # store interem values\n",
        "        train_acc_mean[tree_idx][d_idx] = np.mean(train_perfs)\n",
        "        train_acc_std[tree_idx][d_idx] = np.std(train_perfs)\n",
        "        test_acc_mean[tree_idx][d_idx] = np.mean(test_perfs)\n",
        "        test_acc_std[tree_idx][d_idx] = np.std(test_perfs)\n",
        "\n",
        "# Figure 1\n",
        "plt.figure()\n",
        "sns.heatmap(train_acc_mean, annot=True) # create seaborn heatmap with annotations\n",
        "plt.ylabel(\"Number of trees\")\n",
        "plt.xlabel(\"Max depth of trees\")\n",
        "plt.yticks(ticks=np.arange(0,10), labels=np.arange(10,101,10))\n",
        "plt.xticks(np.arange(0,12), depths)\n",
        "plt.title(\"Random Forest, Train mean absolute errors\")\n",
        "plt.show()\n",
        "\n",
        "# Figure 2\n",
        "plt.figure()\n",
        "sns.heatmap(test_acc_mean, annot=True)\n",
        "plt.ylabel(\"Number of trees\")\n",
        "plt.xlabel(\"Max depth of trees\")\n",
        "plt.yticks(np.arange(0,10), np.arange(10,101,10))\n",
        "plt.xticks(np.arange(0,12), depths)\n",
        "plt.title(\"Random Forest, Test mean absolute errors\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#svm\n",
        "#for svm we use normalized data but since we want the MAE in points\n",
        "# and not in normalized points, we save the original variance and mean.\n",
        "a = PL[\"fs_result\"].values\n",
        "b = PL[\"fs_result\"].values\n",
        "meanV = a.mean()\n",
        "stdevV = b.std()\n",
        "\n",
        "#create a SVR model\n",
        "#degree is only used if polynomial function is given and multiple degrees were tested\n",
        "svr = SVR(kernel='rbf', degree=7, C=0.85)\n",
        "\n",
        "\n",
        "X = PLnormalized.drop('fs_result', axis=1)\n",
        "y = PLnormalized['fs_result']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# train the model on the data\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the data and reformat them to old desired output\n",
        "y_pred = svr.predict(X_train)\n",
        "y_pred_t = svr.predict(X_test)\n",
        "y_train = y_train * stdevV + meanV\n",
        "y_test = y_test * stdevV + meanV\n",
        "y_pred = y_pred * stdevV + meanV\n",
        "y_pred_t = y_pred_t * stdevV + meanV\n",
        "\n",
        "# Get the maximum value between y_train and y_test predictions\n",
        "max_y = max(max(y_train), max(y_test), max(y_pred), max(y_pred_t))\n",
        "\n",
        "# Get the minimum value between y_train and y_test predictions\n",
        "min_y = min(min(y_train), min(y_test), min(y_pred), min(y_pred_t))\n",
        "\n",
        "# Set the limits of both axes to the same range\n",
        "plt.xlim(min_y, max_y)\n",
        "plt.ylim(min_y, max_y)\n",
        "\n",
        "# Scatter plots\n",
        "plt.scatter(y_train, y_pred, color='darkorange', label='train')\n",
        "plt.scatter(y_test, y_pred_t, color='blue', label='test')\n",
        "plt.ylabel(\"y prediction\")\n",
        "plt.xlabel(\"real y\")\n",
        "# Add diagonal line\n",
        "plt.plot([min_y, max_y], [min_y, max_y], color='grey', linestyle='--', label='diagonal')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#print train and test MAE respectively\n",
        "print(mae(y_train, y_pred))\n",
        "print(mae(y_test, y_pred_t))"
      ]
    }
  ]
}